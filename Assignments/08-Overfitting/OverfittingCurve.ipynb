{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "633ff60f",
   "metadata": {},
   "source": [
    "# Overfitting a Curve\n",
    "\n",
    "In this assignment, you will play around with the various models and corresponding parameters. \n",
    "\n",
    "## Questions to Answer\n",
    "\n",
    "Things to try:\n",
    "\n",
    "- **Before you run any code**, make some predictions. What do you expect to see for the different models?\n",
    "    + linear\n",
    "    + quadratic\n",
    "    + cubic\n",
    "    + n-degree polynomial\n",
    "    + ordinary least squares\n",
    "    + neural network\n",
    "- Now run the notebook. What surprised you? What matched your expectations?\n",
    "- Now report on your results with the following:\n",
    "    + Changing the number of degrees in the polynomial model.\n",
    "    + Using a non-zero weight decay.\n",
    "    + Changing the number of layers in the neural network model.\n",
    "    + Changing the number of training samples.\n",
    "- Finally, open the `OverfittingFashionMNIST.ipynb` and see if you can get the neural network to overfit the data (get the bad thing to happen)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686a479",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "jtplot.style(context=\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605dd3de",
   "metadata": {},
   "source": [
    "## Create Fake Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73634f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CubicDataset(Dataset):\n",
    "    def __init__(self, num_samples: int, input_range: tuple[float, float]):\n",
    "\n",
    "        # Internal function to generate fake data\n",
    "        def fake_y(x, add_noise=False):\n",
    "            y = 10 * x ** 3 - 5 * x\n",
    "            return y + torch.randn_like(y) * 0.5 if add_noise else y\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.input_range = input_range\n",
    "\n",
    "        min_x, max_x = input_range\n",
    "\n",
    "        # True curve for plotting purposes\n",
    "        true_N = 100\n",
    "        self.true_X = torch.linspace(min_x, max_x, true_N).reshape(-1, 1)\n",
    "        self.true_y = fake_y(self.true_X)\n",
    "\n",
    "        self.X = torch.rand(self.num_samples).reshape(-1, 1) * (max_x - min_x) + min_x\n",
    "        self.y = fake_y(self.X, add_noise=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def plot(self, model=None, losses=None, poly_deg=None):\n",
    "\n",
    "        # Plot loss curves if given\n",
    "        train_losses = losses[0] if losses else None\n",
    "        valid_losses = losses[1] if losses else None\n",
    "        plot_losses = train_losses != None and len(train_losses) > 1\n",
    "\n",
    "        _, axes = plt.subplots(1, 2, figsize=(16, 8)) if plot_losses else plt.subplots(1, 1, figsize=(8, 8))\n",
    "        ax1: plt.Axes = axes[0] if plot_losses else axes\n",
    "        ax2: plt.Axes | None = axes[1] if plot_losses else None\n",
    " \n",
    "        ax1.plot(self.X, self.y, \"o\", label=\"Noisy Samples\")\n",
    "        ax1.plot(self.true_X, self.true_y, label=\"Baseline Curve\")\n",
    "\n",
    "        # Plot the model's learned regression function\n",
    "        if model:\n",
    "            x = self.true_X.unsqueeze(-1)\n",
    "            x = x.pow(torch.arange(poly_deg + 1)) if poly_deg else x\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_y = model(x)\n",
    "\n",
    "            ax1.plot(self.true_X, pred_y.squeeze(), label=\"Learned Model\")\n",
    "\n",
    "        ax1.set_xlim(self.input_range)\n",
    "        ax1.set_ylim(-5, 5)\n",
    "        ax1.legend()\n",
    "        ax1.set_xlabel(\"x\")\n",
    "        ax1.set_ylabel(\"y\")\n",
    "\n",
    "        # Plot training and validation losses\n",
    "        if plot_losses and ax2:\n",
    "            ax2.plot(torch.linspace(1, num_epochs, len(train_losses)), train_losses, label=\"Training\")\n",
    "            ax2.plot(torch.linspace(1, num_epochs, len(valid_losses)), valid_losses, label=\"Validation\")\n",
    "            ax2.legend()\n",
    "            ax2.set_xlabel(\"Epoch\")\n",
    "            ax2.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples/examples\n",
    "N = 25\n",
    "train_valid_split = [0.8, 0.2]\n",
    "batch_size = N // 4\n",
    "\n",
    "# Range of training data input\n",
    "MIN_X, MAX_X = -1, 1\n",
    "\n",
    "cubic_dataset = CubicDataset(N, (MIN_X, MAX_X))\n",
    "cubic_dataset.plot()\n",
    "\n",
    "train_dataset, valid_dataset = random_split(cubic_dataset, train_valid_split)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "595ba9b6",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(learning_rate, num_epochs, weight_decay, model, params):\n",
    "    # Torch utils\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(params, lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for _ in progress_bar(range(num_epochs)):\n",
    "\n",
    "        # Model can be an nn.Module or a function\n",
    "        if isinstance(model, nn.Module):\n",
    "            model.train()\n",
    "        \n",
    "        for X, y in train_loader:\n",
    "            yhat = model(X)\n",
    "\n",
    "            loss = criterion(yhat, y)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if isinstance(model, nn.Module):\n",
    "            model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in valid_loader:\n",
    "                yhat = model(X)\n",
    "                loss = criterion(yhat, y)\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5bb598",
   "metadata": {},
   "source": [
    "## Train a Linear Model Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 64\n",
    "weight_decay = 0\n",
    "\n",
    "# Model parameters\n",
    "m = torch.randn(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Place parameters into a sequence for torch.optim\n",
    "params = (b, m)\n",
    "\n",
    "# Create simple linear model\n",
    "def model(X):\n",
    "    return m * X + b\n",
    "\n",
    "\n",
    "losses = train_model(learning_rate, num_epochs, weight_decay, model, params)\n",
    "cubic_dataset.plot(model, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9b37f",
   "metadata": {},
   "source": [
    "## Train a Quadratic Model Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd011eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 64\n",
    "weight_decay = 0\n",
    "\n",
    "# Model parameters\n",
    "w2 = torch.randn(1, requires_grad=True)\n",
    "w1 = torch.randn(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Place parameters into a sequence for torch.optim\n",
    "params = (b, w1, w2)\n",
    "\n",
    "# Create simple quadratic model\n",
    "def model(X):\n",
    "    return b + w1 * X + w2 * X ** 2\n",
    "\n",
    "\n",
    "losses = train_model(learning_rate, num_epochs, weight_decay, model, params)\n",
    "cubic_dataset.plot(model, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db198adc",
   "metadata": {},
   "source": [
    "## Train a Cubic Model Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 64\n",
    "weight_decay = 0\n",
    "\n",
    "# Model parameters\n",
    "w3 = torch.randn(1, requires_grad=True)\n",
    "w2 = torch.randn(1, requires_grad=True)\n",
    "w1 = torch.randn(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Place parameters into a sequence for torch.optim\n",
    "params = (b, w1, w2, w3)\n",
    "\n",
    "# Create simple cubic model\n",
    "def model(X):\n",
    "    return b + w1 * X + w2 * X ** 2 + w3 * X ** 3\n",
    "\n",
    "\n",
    "losses = train_model(learning_rate, num_epochs, weight_decay, model, params)\n",
    "cubic_dataset.plot(model, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44575ec",
   "metadata": {},
   "source": [
    "## Train a Polynomial Model Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d58bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 64\n",
    "weight_decay = 0\n",
    "\n",
    "# Model parameters\n",
    "degrees = 50  # 3, 4, 16, 32, 64, 128\n",
    "powers = torch.arange(degrees + 1)\n",
    "params = torch.randn(degrees + 1, requires_grad=True)\n",
    "\n",
    "# Create simple cubic model\n",
    "def model(X):\n",
    "    X_polynomials = X.pow(powers)\n",
    "    return X_polynomials @ params\n",
    "\n",
    "\n",
    "losses = train_model(learning_rate, num_epochs, weight_decay, model, [params])\n",
    "cubic_dataset.plot(model, losses, poly_deg=degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663def1",
   "metadata": {},
   "source": [
    "## Compute Polynomial Model Using Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.tensor([x for x, _ in train_dataset])\n",
    "train_y = torch.tensor([y for _, y in train_dataset])\n",
    "train_X_polynomial = train_X.unsqueeze(-1).pow(powers)\n",
    "\n",
    "# Compute \"optimal\" parameters\n",
    "params = ((train_X_polynomial.T @ train_X_polynomial).inverse() @ train_X_polynomial.T) @ train_y\n",
    "\n",
    "def model(X):\n",
    "    return X @ params\n",
    "\n",
    "\n",
    "# Compute loss\n",
    "mse = nn.functional.mse_loss(train_X_polynomial @ params, train_y)\n",
    "cubic_dataset.plot(model, losses=None, poly_deg=degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3feed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d5fbc",
   "metadata": {},
   "source": [
    "## Train Neural Network Model Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a5473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # The hidden layers include:\n",
    "        # 1. a linear component (computing Z) and\n",
    "        # 2. a non-linear comonent (computing A)\n",
    "        hidden_layers = [\n",
    "            nn.Sequential(nn.Linear(nlminus1, nl), nn.ReLU())\n",
    "            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n",
    "        ]\n",
    "\n",
    "        # For regression we should use a linear output layer\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "weight_decay = 0\n",
    "\n",
    "layer_sizes = (1, 100, 100, 100, 1)\n",
    "\n",
    "model = NeuralNetwork(layer_sizes)\n",
    "summary(model)\n",
    "\n",
    "X = train_X.unsqueeze(-1)\n",
    "\n",
    "losses = train_model(learning_rate, num_epochs, weight_decay, model, model.parameters())\n",
    "cubic_dataset.plot(model, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9927f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.abs().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25feac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupytext --sync OverfittingCurve.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af2d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
