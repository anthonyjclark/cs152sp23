{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "909ee181",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Instructions:\" data-toc-modified-id=\"Instructions:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Instructions:</a></span></li><li><span><a href=\"#Questions-to-Answer\" data-toc-modified-id=\"Questions-to-Answer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Questions to Answer</a></span></li><li><span><a href=\"#Things-to-Try\" data-toc-modified-id=\"Things-to-Try-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Things to Try</a></span></li><li><span><a href=\"#Set-Hyperparameters\" data-toc-modified-id=\"Set-Hyperparameters-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Set Hyperparameters</a></span></li><li><span><a href=\"#Prepare-the-MNIST-Dataset\" data-toc-modified-id=\"Prepare-the-MNIST-Dataset-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Prepare the MNIST Dataset</a></span></li><li><span><a href=\"#Create-a-Neural-Network\" data-toc-modified-id=\"Create-a-Neural-Network-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Create a Neural Network</a></span></li><li><span><a href=\"#Train-Classifier\" data-toc-modified-id=\"Train-Classifier-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Train Classifier</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e08ac",
   "metadata": {},
   "source": [
    "# Mini-Batch SGD Assignment\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "1. Clone this repository (or just pull changes if you already have it).\n",
    "2. Start Jupyter (don't forget to activate conda).\n",
    "3. Duplicate this file so that you can still pull changes without merging.\n",
    "4. Complete the \"Questions to Answer.\"\n",
    "5. Complete the \"Things to Try.\"\n",
    "\n",
    "## Questions to Answer\n",
    "\n",
    "You will answer these questions on gradescope. Try to answer these in your group prior to running or altering any code.\n",
    "\n",
    "1. How could you make this code run \"stochastic gradient descent (SGD)\"?\n",
    "\n",
    "1. How could you make this code run \"batch gradient descent (BGD)\"?\n",
    "\n",
    "1. What is the shape of `train_X`?\n",
    "\n",
    "1. What is the shape of `train_output`?\n",
    "\n",
    "1. What values would you expect to see in the `train_output` tensor?\n",
    "\n",
    "1. What is the shape of `train_Y`?\n",
    "\n",
    "1. What is the purpose of the `with torch.no_grad()` ([documentation](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad)) context manager?\n",
    "\n",
    "1. How do we compute accuracy? Describe the code for doing so.\n",
    "\n",
    "    ~~~python\n",
    "    # Convert network output into predictions (one-hot -> number)\n",
    "    predictions = valid_output.argmax(1)\n",
    "\n",
    "    # Sum up total number that were correct\n",
    "    valid_correct += (predictions == valid_Y).type(torch.float).sum().item()\n",
    "    ~~~\n",
    "\n",
    "1. What happens when you rerun the training cell for additional epochs without rerunning any other cells?\n",
    "\n",
    "1. What happens if you set the device to \"cpu\"?\n",
    "\n",
    "    ~~~python\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = \"cpu\"\n",
    "    ~~~\n",
    "\n",
    "## Things to Try\n",
    "\n",
    "1. Change the hidden layer activation functions to sigmoid. What were the results?\n",
    "\n",
    "1. Change the hidden layer activation functions to [something else](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity). What were the results?\n",
    "\n",
    "1. (Optional) Try adding a [dropout layer](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout) after each activation function. What were the results?\n",
    "\n",
    "1. (Optional) Try changing the dataset to either [KMNIST](https://pytorch.org/vision/0.11/datasets.html#kmnist) or [Fashion-MNIST](https://pytorch.org/vision/0.11/datasets.html#fashion-mnist). What were the results?\n",
    "\n",
    "1. (Optional) Try out the **inference** process.\n",
    "\n",
    "    1. Save the model. \n",
    "    \n",
    "    ~~~python\n",
    "    # All training code above\n",
    "    model_filename = \"A05Model.pth\"\n",
    "    torch.save(model.state_dict(), model_filename)\n",
    "    ~~~\n",
    "    \n",
    "    1. Create a new notebook.\n",
    "    \n",
    "    1. Load the saved model.\n",
    "    \n",
    "    ~~~python\n",
    "    # Need to bring over some code from the training file to make this work\n",
    "    model = NeuralNetwork(layer_sizes)\n",
    "    model.load_state_dict(torch.load(model_filename))\n",
    "    model.eval()\n",
    "    \n",
    "    # Index of a validation example\n",
    "    i = 0\n",
    "\n",
    "    # Example input and output\n",
    "    x, y = valid_loader.dataset[i][0], valid_loader.dataset[i][1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "        prediction = output[0].argmax(0)\n",
    "        print(f\"Prediction : {prediction}\")\n",
    "        print(f\"Target     : {y}\")\n",
    "    ~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50781fa7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7610e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "jtplot.style(context=\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c955e",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b77abda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device.\n"
     ]
    }
   ],
   "source": [
    "# Let's use some shared space for the data (so that we don't have copies\n",
    "# sitting around everywhere)\n",
    "data_path = \"/raid/cs152/cache/pytorch/data\"\n",
    "\n",
    "# Use the GPUs if they are available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using '{device}' device.\")\n",
    "\n",
    "# Model hyperparameters\n",
    "neurons_per_layer = [13, 17]\n",
    "\n",
    "# Mini-Batch SGD hyperparameters\n",
    "batch_size = 256\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a4ad36",
   "metadata": {},
   "source": [
    "## Prepare the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cb6a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data_loaders(path, batch_size, valid_batch_size=0):\n",
    "\n",
    "    # MNIST specific transforms\n",
    "    mnist_mean = (0.1307,)\n",
    "    mnist_std = (0.3081,)\n",
    "    mnist_xforms = Compose([ToTensor(), Normalize(mnist_mean, mnist_std)])\n",
    "\n",
    "    # Training data loader\n",
    "    train_dataset = MNIST(root=path, train=True, download=True, transform=mnist_xforms)\n",
    "\n",
    "    # Set the batch size to N if batch_size is 0\n",
    "    tbs = len(train_dataset) if batch_size == 0 else batch_size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=tbs, shuffle=True)\n",
    "\n",
    "    # Validation data loader\n",
    "    valid_dataset = MNIST(root=path, train=False, download=True, transform=mnist_xforms)\n",
    "\n",
    "    # Set the batch size to N if batch_size is 0\n",
    "    vbs = len(valid_dataset) if valid_batch_size == 0 else valid_batch_size\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=vbs, shuffle=True)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d78186",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/raid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loader, valid_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_mnist_data_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining dataset shape   :\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation dataset shape :\u001b[39m\u001b[38;5;124m\"\u001b[39m, valid_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mget_mnist_data_loaders\u001b[0;34m(path, batch_size, valid_batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m mnist_xforms \u001b[38;5;241m=\u001b[39m Compose([ToTensor(), Normalize(mnist_mean, mnist_std)])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Training data loader\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmnist_xforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Set the batch size to N if batch_size is 0\u001b[39;00m\n\u001b[1;32m     12\u001b[0m tbs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataset) \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m batch_size\n",
      "File \u001b[0;32m/opt/mambaforge/envs/cs152/lib/python3.10/site-packages/torchvision/datasets/mnist.py:99\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/mambaforge/envs/cs152/lib/python3.10/site-packages/torchvision/datasets/mnist.py:179\u001b[0m, in \u001b[0;36mMNIST.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# download files\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename, md5 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources:\n",
      "File \u001b[0;32m/opt/mambaforge/envs/cs152/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mambaforge/envs/cs152/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: makedirs at line 215 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/mambaforge/envs/cs152/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mambaforge/envs/cs152/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/raid'"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader = get_mnist_data_loaders(data_path, batch_size)\n",
    "\n",
    "print(\"Training dataset shape   :\", train_loader.dataset.data.shape)\n",
    "print(\"Validation dataset shape :\", valid_loader.dataset.data.shape)\n",
    "\n",
    "# Notice that each example is 28x28. These are images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a58188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a few images as an example\n",
    "num_to_show = 8\n",
    "images = train_loader.dataset.data[:num_to_show]\n",
    "labels = train_loader.dataset.targets[:num_to_show]\n",
    "\n",
    "fig, axes = plt.subplots(1, num_to_show)\n",
    "\n",
    "for axis, image, label in zip(axes, images, labels):\n",
    "    axis.imshow(image.squeeze(), cmap=\"Greys\")\n",
    "    axis.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "    axis.set_xticks([])\n",
    "    axis.set_yticks([])\n",
    "    axis.set_title(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the underlying data for a single image\n",
    "train_loader.dataset.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can almost make out the \"5\" in the output above\n",
    "# Let's make it a bit more clear\n",
    "image = train_loader.dataset.data[0]\n",
    "image_df = pd.DataFrame(image.squeeze().numpy())\n",
    "image_df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d50c0",
   "metadata": {},
   "source": [
    "## Create a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # The first \"layer\" just rearranges the Nx28x28 input into Nx784\n",
    "        first_layer = nn.Flatten()\n",
    "\n",
    "        # The hidden layers include:\n",
    "        # 1. a linear component (computing Z) and\n",
    "        # 2. a non-linear comonent (computing A)\n",
    "        hidden_layers = [\n",
    "            nn.Sequential(nn.Linear(nlminus1, nl), nn.ReLU())\n",
    "            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n",
    "        ]\n",
    "\n",
    "        # The output layer must be Linear without an activation. See:\n",
    "        #   https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = [first_layer] + hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input layer size depends on the dataset\n",
    "n0 = train_loader.dataset.data.shape[1:].numel()\n",
    "\n",
    "# The output layer size depends on the dataset\n",
    "nL = len(train_loader.dataset.classes)\n",
    "\n",
    "# Preprend the input and append the output layer sizes\n",
    "layer_sizes = [n0] + neurons_per_layer + [nL]\n",
    "model = NeuralNetwork(layer_sizes).to(device)\n",
    "\n",
    "summary(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c4f7f",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f98f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A master bar for fancy output progress\n",
    "mb = master_bar(range(num_epochs))\n",
    "\n",
    "# Information for plots\n",
    "mb.names = [\"Train Loss\", \"Valid Loss\"]\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in mb:\n",
    "\n",
    "    #\n",
    "    # Training\n",
    "    #\n",
    "    model.train()\n",
    "\n",
    "    train_N = len(train_loader.dataset)\n",
    "    num_train_batches = len(train_loader)\n",
    "    train_dataiterator = iter(train_loader)\n",
    "\n",
    "    train_loss_mean = 0\n",
    "\n",
    "    for batch in progress_bar(range(num_train_batches), parent=mb):\n",
    "\n",
    "        # Grab the batch of data and send it to the correct device\n",
    "        train_X, train_Y = next(train_dataiterator)\n",
    "        train_X, train_Y = train_X.to(device), train_Y.to(device)\n",
    "\n",
    "        # Compute the output\n",
    "        train_output = model(train_X)\n",
    "\n",
    "        # Compute loss\n",
    "        train_loss = criterion(train_output, train_Y)\n",
    "\n",
    "        num_in_batch = len(train_X)\n",
    "        tloss = train_loss.item() * num_in_batch / train_N\n",
    "        train_loss_mean += tloss\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        # Compute partial derivatives\n",
    "        model.zero_grad()\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "\n",
    "    #\n",
    "    # Validation\n",
    "    #\n",
    "    model.eval()\n",
    "\n",
    "    valid_N = len(valid_loader.dataset)\n",
    "    num_valid_batches = len(valid_loader)\n",
    "\n",
    "    valid_loss_mean = 0\n",
    "    valid_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # valid_loader is probably just one large batch, so not using progress bar\n",
    "        for valid_X, valid_Y in valid_loader:\n",
    "\n",
    "            valid_X, valid_Y = valid_X.to(device), valid_Y.to(device)\n",
    "\n",
    "            valid_output = model(valid_X)\n",
    "\n",
    "            valid_loss = criterion(valid_output, valid_Y)\n",
    "\n",
    "            num_in_batch = len(valid_X)\n",
    "            vloss = valid_loss.item() * num_in_batch / valid_N\n",
    "            valid_loss_mean += vloss\n",
    "            valid_losses.append(valid_loss.item())\n",
    "\n",
    "            # Convert network output into predictions (one-hot -> number)\n",
    "            predictions = valid_output.argmax(1)\n",
    "\n",
    "            # Sum up total number that were correct\n",
    "            valid_correct += (predictions == valid_Y).type(torch.float).sum().item()\n",
    "\n",
    "    valid_accuracy = 100 * (valid_correct / valid_N)\n",
    "\n",
    "    # Report information\n",
    "    tloss = f\"Train Loss = {train_loss_mean:.4f}\"\n",
    "    vloss = f\"Valid Loss = {valid_loss_mean:.4f}\"\n",
    "    vaccu = f\"Valid Accuracy = {(valid_accuracy):>0.1f}%\"\n",
    "    mb.write(f\"[{epoch+1:>2}/{num_epochs}] {tloss}; {vloss}; {vaccu}\")\n",
    "\n",
    "    # Update plot data\n",
    "    max_loss = max(max(train_losses), max(valid_losses))\n",
    "    min_loss = min(min(train_losses), min(valid_losses))\n",
    "    \n",
    "    x_margin = 0.2\n",
    "    x_bounds = [0 - x_margin, num_epochs + x_margin]\n",
    "\n",
    "    y_margin = 0.1\n",
    "    y_bounds = [min_loss - y_margin, max_loss + y_margin]\n",
    "\n",
    "    valid_Xaxis = torch.linspace(0, epoch + 1, len(train_losses))\n",
    "    valid_xaxis = torch.linspace(1, epoch + 1, len(valid_losses))\n",
    "    graph_data = [[valid_Xaxis, train_losses], [valid_xaxis, valid_losses]]\n",
    "\n",
    "    mb.update_graph(graph_data, x_bounds, y_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d52b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
